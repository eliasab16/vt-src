{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wire Bounding Box Detection - RF-DETR Training\n",
    "Fine-tune RF-DETR for wire detection using Apple Silicon (MPS) acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rfdetr supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rfdetr import RFDETRBase, RFDETRLarge\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# enable MPS fallback for unsupported operations (required for RF-DETR on Apple Silicon)\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon) with CPU fallback for unsupported ops\n"
     ]
    }
   ],
   "source": [
    "# expects COCO format dataset\n",
    "# structure:\n",
    "#   dataset/\n",
    "#     train/\n",
    "#       _annotations.coco.json\n",
    "#       image1.jpg, image2.jpg, ...\n",
    "#     valid/\n",
    "#       _annotations.coco.json\n",
    "#       image1.jpg, image2.jpg, ...\n",
    "#     test/ (optional)\n",
    "#       _annotations.coco.json\n",
    "#       image1.jpg, image2.jpg, ...\n",
    "\n",
    "DATA_DIR = '<data_dir>'\n",
    "OUTPUT_DIR = './checkpoints/rfdetr_jan7'\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 4  # effective batch size = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "LEARNING_RATE = 1e-4\n",
    "RESOLUTION = 560  # input resolution (default 560)\n",
    "\n",
    "# model variant: 'base' or 'large'\n",
    "MODEL_VARIANT = 'base'\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    print('Using MPS (Apple Silicon) with CPU fallback for unsupported ops')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    print('Using CUDA')\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrain weights\n",
      "Loaded RF-DETR Base\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# choose model variant based on config\n",
    "if MODEL_VARIANT == 'large':\n",
    "    model = RFDETRLarge()\n",
    "    print('Loaded RF-DETR Large')\n",
    "else:\n",
    "    model = RFDETRBase()\n",
    "    print('Loaded RF-DETR Base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /Users/elisd/Desktop/vult/data/wires-one-class-bbox-COCO-format\n",
      "Output: ./checkpoints/rfdetr_jan7\n",
      "Device: mps\n",
      "Epochs: 50\n",
      "Batch size: 4 (effective: 16)\n",
      "Learning rate: 0.0001\n",
      "\n",
      "Unable to initialize TensorBoard. Logging is turned off for this session.  Run 'pip install tensorboard' to enable logging.\n",
      "Not using distributed mode\n",
      "git:\n",
      "  sha: N/A, status: clean, branch: N/A\n",
      "\n",
      "Namespace(num_classes=2, grad_accum_steps=4, amp=True, lr=0.0001, lr_encoder=0.00015, batch_size=4, weight_decay=0.0001, epochs=50, lr_drop=100, clip_max_norm=0.1, lr_vit_layer_decay=0.8, lr_component_decay=0.7, do_benchmark=False, dropout=0, drop_path=0.0, drop_mode='standard', drop_schedule='constant', cutoff_epoch=0, pretrained_encoder=None, pretrain_weights='rf-detr-base.pth', pretrain_exclude_keys=None, pretrain_keys_modify_to_load=None, pretrained_distiller=None, encoder='dinov2_windowed_small', vit_encoder_num_layers=12, window_block_indexes=None, position_embedding='sine', out_feature_indexes=[2, 5, 8, 11], freeze_encoder=False, layer_norm=True, rms_norm=False, backbone_lora=False, force_no_pretrain=False, dec_layers=3, dim_feedforward=2048, hidden_dim=256, sa_nheads=8, ca_nheads=16, num_queries=300, group_detr=13, two_stage=True, projector_scale=['P4'], lite_refpoint_refine=True, num_select=300, dec_n_points=2, decoder_norm='LN', bbox_reparam=True, freeze_batch_norm=False, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, cls_loss_coef=1.0, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, aux_loss=True, sum_group_losses=False, use_varifocal_loss=False, use_position_supervised_loss=False, ia_bce_loss=True, dataset_file='roboflow', coco_path=None, dataset_dir='/Users/elisd/Desktop/vult/data/wires-one-class-bbox-COCO-format', square_resize_div_64=True, output_dir='./checkpoints/rfdetr_jan7', dont_save_weights=False, checkpoint_interval=10, seed=42, resume='', start_epoch=0, eval=False, use_ema=True, ema_decay=0.993, ema_tau=100, num_workers=2, device='mps', world_size=1, dist_url='env://', sync_bn=True, fp16_eval=False, encoder_only=False, backbone_only=False, resolution=560, use_cls_token=False, multi_scale=True, expanded_scales=True, do_random_resize_via_padding=False, warmup_epochs=0.0, lr_scheduler='step', lr_min_factor=0.0, early_stopping=False, early_stopping_patience=10, early_stopping_min_delta=0.001, early_stopping_use_ema=False, gradient_checkpointing=True, patch_size=14, num_windows=4, positional_encoding_size=37, mask_downsample_ratio=4, tensorboard=True, wandb=False, project=None, run=None, class_names=['wire'], run_test=True, segmentation_head=False, distributed=False)\n",
      "number of params: 31854308\n",
      "[840]\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "[840]\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[840]\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Get benchmark\n",
      "Start training\n",
      "Grad accum steps:  4\n",
      "Total batch size:  16\n",
      "LENGTH OF DATA LOADER: 276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "UserWarning: The operator 'aten::_upsample_bicubic2d_aa.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:14.)\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4316.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/276]  eta: 2:33:21  lr: 0.000100  class_error: -0.00  loss: 14.0371 (14.0371)  loss_ce: 0.6401 (0.6401)  loss_bbox: 1.0160 (1.0160)  loss_giou: 1.4825 (1.4825)  loss_ce_0: 0.5939 (0.5939)  loss_bbox_0: 1.6259 (1.6259)  loss_giou_0: 1.4471 (1.4471)  loss_ce_1: 0.5547 (0.5547)  loss_bbox_1: 1.3343 (1.3343)  loss_giou_1: 1.6193 (1.6193)  loss_ce_enc: 0.5320 (0.5320)  loss_bbox_enc: 1.5720 (1.5720)  loss_giou_enc: 1.6192 (1.6192)  loss_ce_unscaled: 0.6401 (0.6401)  class_error_unscaled: -0.0000 (-0.0000)  loss_bbox_unscaled: 0.2032 (0.2032)  loss_giou_unscaled: 0.7412 (0.7412)  cardinality_error_unscaled: 1.7500 (1.7500)  loss_ce_0_unscaled: 0.5939 (0.5939)  loss_bbox_0_unscaled: 0.3252 (0.3252)  loss_giou_0_unscaled: 0.7236 (0.7236)  cardinality_error_0_unscaled: 1.7500 (1.7500)  loss_ce_1_unscaled: 0.5547 (0.5547)  loss_bbox_1_unscaled: 0.2669 (0.2669)  loss_giou_1_unscaled: 0.8097 (0.8097)  cardinality_error_1_unscaled: 1.7500 (1.7500)  loss_ce_enc_unscaled: 0.5320 (0.5320)  loss_bbox_enc_unscaled: 0.3144 (0.3144)  loss_giou_enc_unscaled: 0.8096 (0.8096)  cardinality_error_enc_unscaled: 1.7500 (1.7500)  time: 33.3396  data: 6.8149\n"
     ]
    }
   ],
   "source": [
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Dataset: {DATA_DIR}')\n",
    "print(f'Output: {OUTPUT_DIR}')\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS})')\n",
    "print(f'Learning rate: {LEARNING_RATE}')\n",
    "print()\n",
    "\n",
    "model.train(\n",
    "    dataset_dir=DATA_DIR,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "    lr=LEARNING_RATE,\n",
    "    resolution=RESOLUTION,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    device=DEVICE,\n",
    "    use_ema=True,\n",
    "    gradient_checkpointing=True,  # helps with memory on MPS\n",
    "    checkpoint_interval=10,\n",
    ")\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best checkpoint\n",
    "best_checkpoint = Path(OUTPUT_DIR) / 'checkpoint_best_total.pth'\n",
    "\n",
    "if best_checkpoint.exists():\n",
    "    if MODEL_VARIANT == 'large':\n",
    "        trained_model = RFDETRLarge(pretrain_weights=str(best_checkpoint))\n",
    "    else:\n",
    "        trained_model = RFDETRBase(pretrain_weights=str(best_checkpoint))\n",
    "    print(f'Loaded checkpoint: {best_checkpoint}')\n",
    "else:\n",
    "    print(f'Checkpoint not found: {best_checkpoint}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model, image_path, threshold=0.5):\n",
    "    \"\"\"Run prediction on an image and visualize results.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    detections = model.predict(image, threshold=threshold)\n",
    "    \n",
    "    annotated = image.copy()\n",
    "    annotated = sv.BoxAnnotator().annotate(annotated, detections)\n",
    "    \n",
    "    labels = [f'{conf:.2f}' for conf in detections.confidence]\n",
    "    annotated = sv.LabelAnnotator().annotate(annotated, detections, labels)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(annotated)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{len(detections)} detections')\n",
    "    plt.show()\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on an image\n",
    "test_image = '<path_to_test_image>'\n",
    "detections = predict_and_visualize(trained_model, test_image, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Inference (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(model, input_path, output_path, threshold=0.5, display=False):\n",
    "    \"\"\"Process video with trained model.\"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    \n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f'Processing: {width}x{height} @ {fps:.1f} fps ({total_frames} frames)')\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # convert BGR to RGB for prediction\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(rgb_frame)\n",
    "        \n",
    "        # predict\n",
    "        detections = model.predict(pil_image, threshold=threshold)\n",
    "        \n",
    "        # draw boxes\n",
    "        display_frame = frame.copy()\n",
    "        for i in range(len(detections.xyxy)):\n",
    "            x1, y1, x2, y2 = detections.xyxy[i].astype(int)\n",
    "            conf = detections.confidence[i]\n",
    "            \n",
    "            cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            label = f'{conf:.2f}'\n",
    "            cv2.putText(display_frame, label, (x1, y1 - 5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        out.write(display_frame)\n",
    "        frame_count += 1\n",
    "        \n",
    "        if frame_count % 100 == 0:\n",
    "            print(f'Progress: {frame_count}/{total_frames} ({100*frame_count/total_frames:.1f}%)')\n",
    "        \n",
    "        if display:\n",
    "            cv2.imshow('Output', display_frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    if display:\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    print(f'Done! Saved to {output_path}')\n",
    "    return frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrain weights\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path='/Users/elisd/Desktop/vult/models/trained_models/bounding_box_rfdetr_jan7/rfdetr_jan7/checkpoint_best_total.pth'\n",
    "trained_model = RFDETRBase(pretrain_weights=str(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 800x600 @ 30.0 fps (1770 frames)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The operator 'aten::_upsample_bicubic2d_aa.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:14.)\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4316.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Saved to /Users/elisd/Desktop/overhead_test.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process a video (uncomment to use)\n",
    "process_video(\n",
    "    trained_model,\n",
    "    input_path='/Users/elisd/Desktop/overhead.mp4',\n",
    "    output_path='/Users/elisd/Desktop/overhead_test.mp4',\n",
    "    threshold=0.7,\n",
    "    display=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
